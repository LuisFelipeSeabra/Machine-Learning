
"""

Neste arquivo foram utilizados os algoritmos: 
Random Forest
SVC
Naive Bayes
KNN
Regressão Logistica
árvore 
Redes Neurais(Keras)

Utilizou-se apenas o dataset de treino, aplicou-se train_test_split e as metricas utilizadas foram: accuracy_score e confusion_matrix

"""
#importação da base
import pandas as pd
base = pd.read_csv('train.csv')
import numpy as np
##########Graficos#############\
import matplotlib.pyplot as plt

train = base
#Barchart
def bar_chart(feature):
    survived = train[train['Survived']==1][feature].value_counts()
    notsurvived = train[train['Survived']==0][feature].value_counts()
    df =pd.DataFrame([survived,notsurvived])
    df.index = ['survived','notsurvived']
    df.plot(kind = 'bar', stacked = True, figsize =(9,5))

bar_chart('Pclass')
bar_chart('Sex')


##########Matriz de correlação#############
import seaborn as sns
plt.figure(figsize=(8,4), dpi = 100, facecolor = 'Pink')
matriz_correl = sns.heatmap(train[['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',
       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].corr(), annot = True, fmt='.2f', cmap= 'coolwarm',linewidths=.5)
plt.text(5,6.5, "Matriz de correlação", fontsize = 12, color='Black', fontstyle='italic')


##########FacetGrid#############
graf_age = sns.FacetGrid(train, col='Survived')
graf_age = graf_age.map(sns.distplot,'Age')

g = sns.FacetGrid(train, col='Survived',  row='Pclass')
g = g.map(plt.hist, 'Age')

g = sns.FacetGrid(train, col='Survived',  row='Sex')
g = g.map(plt.hist, 'Age')

g = sns.FacetGrid(train, col='Survived',  row='Sex')
g = g.map(plt.hist, 'Pclass')


g = sns.FacetGrid(train, col='Survived',  row='Pclass')
g = g.map(plt.scatter, 'Age', 'Fare')

##########Kde plot#############

distribuicao = sns.kdeplot(train['Age'][(train['Survived']==0 & train['Age'].notnull())], color = 'green', shade = True)
distribuicao = sns.kdeplot(train['Age'][(train['Survived']==1 & train['Age'].notnull())], color = 'red', shade = True)
distribuicao.set_xlabel('Age')
distribuicao.set_ylabel('Frequency')
distribuicao = distribuicao.legend(['Não sobreviveu','Sobreviveu'])



##########Kde plot#############

fig = plt.figure(figsize=(10,10))
ax1 = plt.subplot(2,1,1)
ax1 = sns.countplot(x = 'Pclass', hue = 'Survived', data = train)
ax1.set_title('Survival Rate')
ax1.set_xticklabels(['1 Upper','2 Middle','3 Lower'])
ax1.set_ylim(0,400)
ax1.set_xlabel('Ticket Class')
ax1.set_ylabel('Count')
ax1.legend(['No','yes'])



########Percent Survived####################
fig = plt.figure(figsize=(10,10))
ax2 = plt.subplot(2,1,2)
sns.pointplot(x='Pclass',y='Survived',data = train)
ax2.set_xlabel('Ticket Class')
ax2.set_ylabel('Percent Survived')
ax2.set_title('Survival Percentage')


#############Survival rate vs Siblings or Spouse on Board################

g= sns.factorplot(x='SibSp', y='Survived',data =train,kind = 'bar', palette = 'muted',size = 5)

g = sns.factorplot(x='Parch',y='Survived',data= train,kind = 'bar',size =5,palette='muted')



fig = plt.figure(figsize=(10,5))
sns.swarmplot(x='Pclass',y='Fare',data=train,hue= 'Survived')


############Valores nulos#############
sns.heatmap(train.isnull())

#########################Tratamento dos dados#####################################



#substituindo valores nulos por 'S'

datamissing = [base]
for data in datamissing:
        base['Embarked'] = base['Embarked'].fillna('S')



# Substituindo letras por numeros

def embarked(s):
    if s =='S' :
        return 1
    elif s == 'C':
        return 2
    else:
        return 3

base['Embarked']=base['Embarked'].apply(embarked)
       
     
#SEX

gender = {'male': 0, 'female': 1}
for data in base:
    if data == 'Sex':
        base['Sex'] = base['Sex'].map(gender)

#exclui coluna B_Stance
#base.drop(base['B_Stance'], axis = 0) # não funcionou?????
#del base['Name']
del base['PassengerId']
del base['Ticket']
del base['Cabin']


data = [base]
Title = {'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5}

for dataset in data:
    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\.',expand = False)
#Replace title with more common one
    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr', 
                                                'Major','Rev','Sir','Jonkheer','Dona'],'Rare')
    dataset['Title'] = dataset['Title'].replace('Ms','Miss')
    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')
    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')
    dataset['Title'] = dataset['Title'].map(Title)
    dataset['Title'] = dataset['Title'].fillna(0)
dataset['Title'].unique()
del base['Name']




#excluindo linhas de "Embarked" que possuam valores nulos
#base = base.dropna(subset=['Embarked'])

#localizando registros nulos e alterando pela média
#base.loc[pd.isnull(base['B_avg_BODY_att']),'B_avg_BODY_att'] = base['B_avg_BODY_att'][base.B_avg_BODY_att>0].mean()
colunas = []        
colunas = base.columns 
colunas = np.asarray(colunas)
for i in range(colunas.size):   
        if base[colunas[i]].isnull().sum() > 0:
           # print( colunas[i])
            base.loc[pd.isnull(base[colunas[i]]),colunas[i]] =  base[colunas[i]][base[colunas[i]]>0].mean()



# AGE
            
data = [base]
for dataset in data:
    dataset['Age'] = dataset['Age'].astype(int)
    dataset.loc[dataset['Age'] <=11,'Age'] =0
    dataset.loc[(dataset['Age'] >11) & (dataset['Age']<=20),'Age'] = 1
    dataset.loc[(dataset['Age'] >20) & (dataset['Age']<=25),'Age'] = 2
    dataset.loc[(dataset['Age'] >25) & (dataset['Age']<=30),'Age'] = 3
    dataset.loc[(dataset['Age'] >30) & (dataset['Age']<=38),'Age'] = 4
    dataset.loc[(dataset['Age'] >38) & (dataset['Age']<=50),'Age'] = 5
    dataset.loc[(dataset['Age'] >50) & (dataset['Age']<=62),'Age'] = 6
    dataset.loc[dataset['Age']>62,'Age'] = 7

base['Age'] = base['Age'].astype(int)

base['Age'].value_counts()

# FARE
    
from sklearn.preprocessing import LabelEncoder
base['Fare'] = pd.qcut(base['Fare'],4)
lbl = LabelEncoder()
base['Fare'] = lbl.fit_transform(base['Fare'])
base['Fare'] = base['Fare'].astype(int)



data = [base]
for dataset in data:
    dataset['Relative'] = dataset['SibSp']+dataset['Parch']
    
#del base['SibSp']
#del base['Parch']


#criando nova coluna
base['Survived2'] = base['Survived']
del base['Survived']
base.rename(columns={'Survived2':'Survived'}, inplace = True)



# Apagar registros com problema 
#base.drop(base[base.age < 0].index,inplace=True)
#traz a média de age quando maior que zero 
#base['age'][base.age>0].mean()
#Preencher manualmente(substituir os valores com a média) 
#base.loc[base.age<0, 'age'] = base['age'][base.age>0].mean()

#----------------------------------------------

#divisão da base em previsores e classe
previsores = base.iloc[:,0:9].values
classe = base.iloc[:,9].values

#trasformação de variáveis categóricas em dados numéricos para previsores
#from sklearn.preprocessing import LabelEncoder, OneHotEncoder
#labelencoder_previsores = LabelEncoder()

#previsores[:,1] = labelencoder_previsores.fit_transform(previsores[:,1])
#previsores[:,6] = labelencoder_previsores.fit_transform(previsores[:,6])
 
#print(previsores[0:2,0:73])

# Criação de variáveis dummy (OneHotEncoder)
#onehotencoder = OneHotEncoder(categorical_features=[1])
#onehotencoder = OneHotEncoder(categorical_features=[6])
#previsores = onehotencoder.fit_transform(previsores).toarray()

#trasformação de variáveis categóricas em dados numéricos para classe
#labelencoder_classe = LabelEncoder()
#classe = labelencoder_classe.fit_transform(classe)

#escalonamento- deixar os valores em esclas parecidas -> melhor desempenho
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
previsores = scaler.fit_transform(previsores)

#treinamento e teste
from sklearn.model_selection import train_test_split
previsores_treinamento,previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size = 0.25, random_state=0)

#importação da biblioteca (SVM)
from sklearn.svm import SVC
classificador = SVC(kernel = 'rbf', random_state= 1, C = 1)
classificador.fit(previsores_treinamento,classe_treinamento)
previsao_SVC = classificador.predict(previsores_teste)


#Resultado (SVM)
from sklearn.metrics import accuracy_score,confusion_matrix
precisao_SVC = accuracy_score(previsao_SVC,classe_teste)
matriz_SVC = confusion_matrix(previsao_SVC,classe_teste)

#### OBS: sem Dummy:0.7238805970149254

#Resultado (REGRESSÂO LOGISTICA)
from sklearn.linear_model import LogisticRegression
classificador = LogisticRegression(random_state=1)
classificador.fit(previsores_treinamento,classe_treinamento)
previsoes_REGLOG = classificador.predict(previsores_teste)

#resultado (REGRESSÂO LOGISTICA)
from sklearn.metrics import accuracy_score, confusion_matrix
precisao_REGLOG = accuracy_score(previsoes_REGLOG, classe_teste)
matriz_REGLOG = confusion_matrix(previsoes_REGLOG,classe_teste)


#aplicação do algorítmo (RANDOM FOREST)
from sklearn.ensemble import RandomForestClassifier
classificador = RandomForestClassifier(n_estimators = 50, criterion = 'entropy')
classificador.fit(previsores_treinamento,classe_treinamento)
previsao_RF =  classificador.predict(previsores_teste)

#Resultado  (RANDOM FOREST)
from sklearn.metrics import confusion_matrix, accuracy_score
precisao_RF = accuracy_score(classe_teste,previsao_RF)
matriz_RF = confusion_matrix(classe_teste,previsao_RF)

#importação para (NAIVE BAYES)
from sklearn.naive_bayes import GaussianNB
classificador = GaussianNB()
classificador.fit(previsores_treinamento,classe_treinamento)
previsoes_BAYES = classificador.predict(previsores_teste)

# comparação de resultado (NAIVE BAYES)
from sklearn.metrics import confusion_matrix, accuracy_score
precisao_BAYES = accuracy_score(classe_teste,previsoes_BAYES)
matriz_BAYES = confusion_matrix(classe_teste,previsoes_BAYES)


# importação da bibliotca(KNN)
from sklearn.neighbors import KNeighborsClassifier
classificador = KNeighborsClassifier(n_neighbors=5,metric='minkowski',  p=2)
classificador.fit(previsores_treinamento,classe_treinamento)
previsoes_KNN = classificador.predict(previsores_treinamento)

#Resulado(KNN)
from sklearn.metrics import confusion_matrix, accuracy_score
precisao_KNN = accuracy_score(classe_treinamento,previsoes_KNN)
matriz_KNN = confusion_matrix(classe_treinamento,previsoes_KNN)


# aplicando o algorítimo -> 
from sklearn.tree import DecisionTreeClassifier
classificador = DecisionTreeClassifier(criterion = 'entropy', random_state=0)
classificador.fit(previsores_treinamento,classe_treinamento)
previsoes_TREE = classificador.predict(previsores_teste)

from sklearn.metrics import confusion_matrix, accuracy_score
precisao_TREE = accuracy_score(classe_teste,previsoes_TREE)
matriz_TREE = confusion_matrix(classe_teste, previsoes_TREE)



#Resultado (REDES NEURAIS KERAS)
from  tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

classificador = Sequential()
classificador.add(Dense(units = 5, activation = 'relu', input_dim = 9)) #imput_dim -> neurônios na camada de entrada
classificador.add(Dense(units = 5, activation = 'relu', ))
classificador.add(Dense(units = 1, activation = 'sigmoid'))
classificador.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = ['accuracy'])

classificador.fit(previsores_treinamento, classe_treinamento, batch_size = 10, nb_epoch = 10)
previsao_KERAS = classificador.predict(previsores_teste)
previsao_KERAS = previsao_KERAS > 0.5

from sklearn.metrics import accuracy_score, confusion_matrix
precisao_KERAS = accuracy_score(previsao_KERAS, classe_teste)
matriz_KERAS= confusion_matrix(previsao_KERAS,classe_teste)





