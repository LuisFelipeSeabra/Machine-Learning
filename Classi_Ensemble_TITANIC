"""
Para esta técnica de Ensemble utilizou-se os classificadores baseado nos algoritmos de KNN, Random Forest e SVC, onde foram dados 
pesos iguais a todos os algoritmos. Ao final da contabilização para 'vive'(1) e 'nao_vive'(0) é adicionado o valor de 1 ou 0 na lista
resul_ensemble e posteriormente verificada a acurácia do classificador.

Score dos classificadores:
resultado_svm:0.8462401795735129
resultado_randomForest: 0.9079685746352413
resultado_knn: 0.8428731762065096

Accuracy do ensemble: 0.8827751196172249


"""

#importação da base
import pandas as pd
base = pd.read_csv('train.csv')

import numpy as np
import pickle
#########################Tratamento dos dados#####################################

#substituindo valores nulos por 'S'

datamissing = [base]
for data in datamissing:
        base['Embarked'] = base['Embarked'].fillna('S')



# Substituindo letras por numeros

def embarked(s):
    if s =='S' :
        return 1
    elif s == 'C':
        return 2
    else:
        return 3

base['Embarked']=base['Embarked'].apply(embarked)
       
     
#SEX

gender = {'male': 0, 'female': 1}
for data in base:
    if data == 'Sex':
        base['Sex'] = base['Sex'].map(gender)

#exclui coluna B_Stance
#base.drop(base['B_Stance'], axis = 0) # não funcionou?????
#del base['Name']
del base['PassengerId']
del base['Ticket']
del base['Cabin']


data = [base]
Title = {'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5}

for dataset in data:
    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\.',expand = False)
#Replace title with more common one
    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr', 
                                                'Major','Rev','Sir','Jonkheer','Dona'],'Rare')
    dataset['Title'] = dataset['Title'].replace('Ms','Miss')
    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')
    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')
    dataset['Title'] = dataset['Title'].map(Title)
    dataset['Title'] = dataset['Title'].fillna(0)
dataset['Title'].unique()
del base['Name']

#excluindo linhas de "Embarked" que possuam valores nulos
#base = base.dropna(subset=['Embarked'])

#localizando registros nulos e alterando pela média
#base.loc[pd.isnull(base['B_avg_BODY_att']),'B_avg_BODY_att'] = base['B_avg_BODY_att'][base.B_avg_BODY_att>0].mean()
colunas = []        
colunas = base.columns 
colunas = np.asarray(colunas)
for i in range(colunas.size):   
        if base[colunas[i]].isnull().sum() > 0:
           # print( colunas[i])
            base.loc[pd.isnull(base[colunas[i]]),colunas[i]] =  base[colunas[i]][base[colunas[i]]>0].mean()



# AGE
            
data = [base]
for dataset in data:
    dataset['Age'] = dataset['Age'].astype(int)
    dataset.loc[dataset['Age'] <=11,'Age'] =0
    dataset.loc[(dataset['Age'] >11) & (dataset['Age']<=20),'Age'] = 1
    dataset.loc[(dataset['Age'] >20) & (dataset['Age']<=25),'Age'] = 2
    dataset.loc[(dataset['Age'] >25) & (dataset['Age']<=30),'Age'] = 3
    dataset.loc[(dataset['Age'] >30) & (dataset['Age']<=38),'Age'] = 4
    dataset.loc[(dataset['Age'] >38) & (dataset['Age']<=50),'Age'] = 5
    dataset.loc[(dataset['Age'] >50) & (dataset['Age']<=62),'Age'] = 6
    dataset.loc[dataset['Age']>62,'Age'] = 7

base['Age'] = base['Age'].astype(int)

base['Age'].value_counts()

# FARE
    
from sklearn.preprocessing import LabelEncoder
base['Fare'] = pd.qcut(base['Fare'],5)
lbl = LabelEncoder()
base['Fare'] = lbl.fit_transform(base['Fare'])
    

base['Fare'] = base['Fare'].astype(int)

#criando nova coluna
base['Survived2'] = base['Survived']
del base['Survived']
base.rename(columns={'Survived2':'Survived'}, inplace = True)

#divisão da base em previsores e classe
previsores = base.iloc[:,0:8].values
classe = base.iloc[:,8].values


#escalonamento- deixar os valores em esclas parecidas -> melhor desempenho
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
previsores = scaler.fit_transform(previsores)


#importação
svm = pickle.load(open('Svm_finalizado2.sav', 'rb'))
randomForest = pickle.load(open('Random_Forest_finalizado2.sav', 'rb'))
knn = pickle.load(open('Knnt_finalizado2.sav', 'rb'))

#Score
resultado_svm = svm.score(previsores,classe)
resultado_randomForest = randomForest.score(previsores,classe)
resultado_knn = knn.score(previsores,classe)

######__________________________"CARREGANDO BASE DE TESTE"________________________________########
#Verificando com a base de teste
base_test = pd.read_csv('test.csv')


base = base_test
#substituindo valores nulos por 'S'
datamissing = [base]
for data in datamissing:
        base['Embarked'] = base['Embarked'].fillna('S')

# Substituindo letras por numeros

def embarked(s):
    if s =='S' :
        return 1
    elif s == 'C':
        return 2
    else:
        return 3

base['Embarked']=base['Embarked'].apply(embarked)
       
#SEX
gender = {'male': 0, 'female': 1}
for data in base:
    if data == 'Sex':
        base['Sex'] = base['Sex'].map(gender)

#exclui coluna B_Stance
#base.drop(base['B_Stance'], axis = 0) # não funcionou?????
#del base['Name']
        
del base['PassengerId']
del base['Ticket']
del base['Cabin']

data = [base]
Title = {'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5}

for dataset in data:
    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\.',expand = False)
#Replace title with more common one
    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr', 
                                                'Major','Rev','Sir','Jonkheer','Dona'],'Rare')
    dataset['Title'] = dataset['Title'].replace('Ms','Miss')
    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')
    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')
    dataset['Title'] = dataset['Title'].map(Title)
    dataset['Title'] = dataset['Title'].fillna(0)
dataset['Title'].unique()
del base['Name']

#localizando registros nulos e alterando pela média
#base.loc[pd.isnull(base['B_avg_BODY_att']),'B_avg_BODY_att'] = base['B_avg_BODY_att'][base.B_avg_BODY_att>0].mean()
colunas = []        
colunas = base.columns 
colunas = np.asarray(colunas)
for i in range(colunas.size):   
        if base[colunas[i]].isnull().sum() > 0:
           # print( colunas[i])
            base.loc[pd.isnull(base[colunas[i]]),colunas[i]] =  base[colunas[i]][base[colunas[i]]>0].mean()

# AGE
data = [base]
for dataset in data:
    dataset['Age'] = dataset['Age'].astype(int)
    dataset.loc[dataset['Age'] <=11,'Age'] =0
    dataset.loc[(dataset['Age'] >11) & (dataset['Age']<=20),'Age'] = 1
    dataset.loc[(dataset['Age'] >20) & (dataset['Age']<=25),'Age'] = 2
    dataset.loc[(dataset['Age'] >25) & (dataset['Age']<=30),'Age'] = 3
    dataset.loc[(dataset['Age'] >30) & (dataset['Age']<=38),'Age'] = 4
    dataset.loc[(dataset['Age'] >38) & (dataset['Age']<=50),'Age'] = 5
    dataset.loc[(dataset['Age'] >50) & (dataset['Age']<=62),'Age'] = 6
    dataset.loc[dataset['Age']>62,'Age'] = 7

base['Age'] = base['Age'].astype(int)
base['Age'].value_counts()

# FARE
from sklearn.preprocessing import LabelEncoder
base['Fare'] = pd.qcut(base['Fare'],5)
lbl = LabelEncoder()
base['Fare'] = lbl.fit_transform(base['Fare'])

base['Fare'] = base['Fare'].astype(int)


base_test = base

base_test = np.asarray(base_test)
base_test = base_test.reshape(-1,8)
base_test = scaler.fit_transform(base_test)
base_test = base_test.reshape(-1,8)

#Predição de quem sobrevive ou não
resposta_svm = svm.predict(base_test)
resposta_randomForest = randomForest.predict(base_test)
resposta_knn = knn.predict(base_test)


# contadores:
vive = 0
nao_vive = 0
resul_ensemble = []

for i in range(base_test[:,1].size):
    if resposta_svm[i] ==1:
        vive +=1
    else: 
        nao_vive +=1
    
    if resposta_randomForest[i] ==1:
        vive +=1
    else: 
        nao_vive +=1
    
    if resposta_knn[i] ==1:
        vive +=1
    else: 
        nao_vive +=1
    
    if vive > nao_vive:
        resul_ensemble.append(1)
        vive = 0
        nao_vive = 0
    else:
        resul_ensemble.append(0)
        vive = 0
        nao_vive = 0


######____________________________________________________________________########
                                    #Resultado
######____________________________________________________________________########
        
final = pd.read_csv('gender_submission.csv')
final =final['Survived']
#Resultado
from sklearn.metrics import confusion_matrix, accuracy_score
precisao = accuracy_score(final,resul_ensemble)
matriz = confusion_matrix(final,resul_ensemble)




