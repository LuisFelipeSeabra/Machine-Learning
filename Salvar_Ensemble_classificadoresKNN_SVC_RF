"""

Salvando os classificadores utilizando os algoritmos Random Forest, SVM e KNN. Tais classificadores serão utilizados no Ensemble.

"""
#importação da base
import pandas as pd
base = pd.read_csv('train.csv')

import numpy as np

#########################Tratamento dos dados#####################################

#substituindo valores nulos por 'S'

datamissing = [base]
for data in datamissing:
        base['Embarked'] = base['Embarked'].fillna('S')



# Substituindo letras por numeros

def embarked(s):
    if s =='S' :
        return 1
    elif s == 'C':
        return 2
    else:
        return 3

base['Embarked']=base['Embarked'].apply(embarked)
       
     
#SEX

gender = {'male': 0, 'female': 1}
for data in base:
    if data == 'Sex':
        base['Sex'] = base['Sex'].map(gender)

#exclui coluna B_Stance
#base.drop(base['B_Stance'], axis = 0) # não funcionou?????
#del base['Name']
del base['PassengerId']
del base['Ticket']
del base['Cabin']


data = [base]
Title = {'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5}

for dataset in data:
    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\.',expand = False)
#Replace title with more common one
    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr', 
                                                'Major','Rev','Sir','Jonkheer','Dona'],'Rare')
    dataset['Title'] = dataset['Title'].replace('Ms','Miss')
    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')
    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')
    dataset['Title'] = dataset['Title'].map(Title)
    dataset['Title'] = dataset['Title'].fillna(0)
dataset['Title'].unique()
del base['Name']








#excluindo linhas de "Embarked" que possuam valores nulos
#base = base.dropna(subset=['Embarked'])

#localizando registros nulos e alterando pela média
#base.loc[pd.isnull(base['B_avg_BODY_att']),'B_avg_BODY_att'] = base['B_avg_BODY_att'][base.B_avg_BODY_att>0].mean()
colunas = []        
colunas = base.columns 
colunas = np.asarray(colunas)
for i in range(colunas.size):   
        if base[colunas[i]].isnull().sum() > 0:
           # print( colunas[i])
            base.loc[pd.isnull(base[colunas[i]]),colunas[i]] =  base[colunas[i]][base[colunas[i]]>0].mean()



# AGE
            
data = [base]
for dataset in data:
    dataset['Age'] = dataset['Age'].astype(int)
    dataset.loc[dataset['Age'] <=11,'Age'] =0
    dataset.loc[(dataset['Age'] >11) & (dataset['Age']<=20),'Age'] = 1
    dataset.loc[(dataset['Age'] >20) & (dataset['Age']<=25),'Age'] = 2
    dataset.loc[(dataset['Age'] >25) & (dataset['Age']<=30),'Age'] = 3
    dataset.loc[(dataset['Age'] >30) & (dataset['Age']<=38),'Age'] = 4
    dataset.loc[(dataset['Age'] >38) & (dataset['Age']<=50),'Age'] = 5
    dataset.loc[(dataset['Age'] >50) & (dataset['Age']<=62),'Age'] = 6
    dataset.loc[dataset['Age']>62,'Age'] = 7

base['Age'] = base['Age'].astype(int)

base['Age'].value_counts()

# FARE
    
from sklearn.preprocessing import LabelEncoder
base['Fare'] = pd.qcut(base['Fare'],5)
lbl = LabelEncoder()
base['Fare'] = lbl.fit_transform(base['Fare'])
    

base['Fare'] = base['Fare'].astype(int)

#criando nova coluna
base['Survived2'] = base['Survived']
del base['Survived']
base.rename(columns={'Survived2':'Survived'}, inplace = True)



# Apagar registros com problema 
#base.drop(base[base.age < 0].index,inplace=True)
#traz a média de age quando maior que zero 
#base['age'][base.age>0].mean()
#Preencher manualmente(substituir os valores com a média) 
#base.loc[base.age<0, 'age'] = base['age'][base.age>0].mean()

#----------------------------------------------

#divisão da base em previsores e classe
previsores = base.iloc[:,0:8].values
classe = base.iloc[:,8].values

#trasformação de variáveis categóricas em dados numéricos para previsores
#from sklearn.preprocessing import LabelEncoder, OneHotEncoder
#labelencoder_previsores = LabelEncoder()

#previsores[:,1] = labelencoder_previsores.fit_transform(previsores[:,1])
#previsores[:,6] = labelencoder_previsores.fit_transform(previsores[:,6])
 
#print(previsores[0:2,0:73])

# Criação de variáveis dummy (OneHotEncoder)
#onehotencoder = OneHotEncoder(categorical_features=[1])
#onehotencoder = OneHotEncoder(categorical_features=[6])
#previsores = onehotencoder.fit_transform(previsores).toarray()

#trasformação de variáveis categóricas em dados numéricos para classe
#labelencoder_classe = LabelEncoder()
#classe = labelencoder_classe.fit_transform(classe)

#escalonamento- deixar os valores em esclas parecidas -> melhor desempenho
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
previsores = scaler.fit_transform(previsores)


#aplicação do algorítmo
from sklearn.ensemble import RandomForestClassifier
classificadorRandomForest = RandomForestClassifier(n_estimators = 50, criterion = 'entropy')
classificadorRandomForest.fit(previsores,classe)


from sklearn.svm import SVC
#criação do classificador
classificadorSvm = SVC(kernel = 'rbf', random_state= 1, C = 1)
classificadorSvm.fit(previsores,classe)


from sklearn.neighbors import KNeighborsClassifier
#criação doclassificador
classificadorKnn = KNeighborsClassifier(n_neighbors=5,metric='minkowski',  p=2)
classificadorKnn.fit(previsores,classe)


import pickle
pickle.dump(classificadorRandomForest, open('Random_Forest_finalizado2.sav','wb'))

pickle.dump(classificadorSvm, open('Svm_finalizado2.sav','wb'))

pickle.dump(classificadorKnn, open('Knnt_finalizado2.sav','wb'))

